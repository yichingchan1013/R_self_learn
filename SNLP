## Statistical Natural Language Processing

Chinese: file:///Users/dtseng02/Documents/Dennis/R%E8%AE%80%E6%9C%AC/snlp.pdf
English: file:///Users/dtseng02/Documents/Dennis/R%E8%AE%80%E6%9C%AC/%E7%B5%B1%E8%A8%88%E8%87%AA%E7%84%B6%E8%AA%9E%E8%A8%80%E8%99%95%E7%90%86%E5%9F%BA%E7%A4%8E%E4%B8%AD%E6%96%87%E7%89%88.pdf

### Chapter 1 Introduction     
原本的語言學會去尋找語言的規則譬如文法，再來會試圖分辨什麼是好的vs.不好的，但嘗試做這件事情面臨到的問題是語言要跟上人類的需求，
所以常常會有文法上的變化，所以前述的規則就很難一體適用，而這本書的做法不僅僅做分拆，更想做的事情其實是找pattern
（跟前面的做法比起來，前者比較像是嘗試找出規範性的規則，但遇到問題了，後者則是比較敘述性，直接看結果），
所以就利用數學算術以及統計跟機率來計算語言的元素

rationalist: 給機器一定的知識以及推理系統就可以像人類一樣
empiricist: 跟r差在程度差距，也假定人有一些基本的運作系統，對怎樣開展推理或運作有特別的偏好，但是相對前者比較抽象/高階，r會直接假設人腦預設了一套語言規則系統，但e只認為人可能會有發現關聯、辨認模式、歸納法等，透過input可能是教學可能是環境耳濡目染，讓人可以學會語言

就NLP而言，r比較偏向給定一些規則接著讓機器運作，e比較偏向給一些大方向/函數，細部的操作/參數則由統計或機器學習決定

E很強調以corpus為背景學習text的意涵/用途（因為沒辦法從真實世界直接學習，所以會看特定文本底下的text）
I-language: language module of the human mind
E-language: data such as texts (the E-language)

r-linguistic competence 語言能力: reflects the knowledge of language structure that is assumed to be in the mind of a native speaker; one can
isolate linguistic competence and describe it in isolation
e-linguistic performance 語言表現: in the world, which is affected by all sorts of things such as memory limitations and distracting
noises in the environment; reject this notion and want to describe actual use of language.

上面大概是講說r認為有所謂的語言能力，人內建能力可以理解語言架構，e則關注實際語言被實際使用的狀況
最後小結論，r就是設定規則（先驗的）看文句，e則是用統計（後驗的）看文句

兩個語言學家應該問的問題
-What kinds of things do people say? 
-What do these things say/ask/request about the world?
看文本內的pattern其實就可以讓我們理解文句的句法架構，所以NLP就在處理這塊，也就回答了第一個問題
又跳回去講R會怎麼回答第一個問題，R會用文法來講，值得注意的是，grammality純粹用來說這句話是不是well-formed，但判斷標準卻不是以人們會不會這樣說來決定，也就是說利用某些規則去評價，但這些規則未必貼合現實（譬如說非母語使用者會講一些文法對的話，但講出來很怪）也就是說，有些句子文法對有些句子文法錯，so what，這個（文法對錯）的資訊到底告訴了我們什麼？（既然不是用現實情況來評價的話）

#語言的non-categorical(categorical: 明確的) 現象
正因為語言變遷普遍來說是隨時間慢慢演化的，因此若要觀察這樣的趨勢不是直接說喔喔這個詞是什麼屬性有什麼用途，較好的作法應該是觀察使用頻率以及和其他詞彙的關係，也就是說應該是用統計方法而非明確的觀察判斷，更激進一點說：把Language and cognition以 probabilistic phenomena看待
批評者說，統計不能告訴我們一段話的意義是什麼，但是作者回應，統計會問要怎麼定義一段話的意義？從統計NLP的角度來看，意義會在於distribution of contexts over which words and utterances are used.也呼應前述的"the meaning of a word is defined by the circumstances of its use"

a practical NLP system must be good at making disambiguation decisions of word sense, word category, syntactic structure, and
semantic scope. But the goal of maximizing coverage while minimizing resultant ambiguity is fundamentally inconsistent with  symbolic NLP
systems, where extending the coverage of the grammar to obscure constructions simply increases the number of undesired parses for common sentences and vice versa.
A Statistical NLP approach seeks to solve these problems by automatically learning lexical and structural preferences from corpora. Rather than parsing solely using syntactic categories, such as part of speech labels, we recognize that there is a lot of information in the relationships between words, that is, which words tend to group with each other.
In particular, the use of statistical models offers a good solution to the ambiguity problem: statistical models are robust,
generalize well, and behave gracefully in the presence of errors and new data.

第一個問題等同問
Rather than starting off by dividing sentences into grammatical and ungrammatical ones, we instead ask, 
“What are the common patterns that occur in language use?” The major tool which we use to identify these patterns is counting things, 
otherwise known as statistics, and so the scientific foundation of the book is found in probability theory.

* 其他語言學家會感興趣的問題   
> How many words are there in the text?
1. How many word tokens there are: 
2. How many different words (how many word types): 
tokens, individual occurrences of something
types, the different things present
the ratio of tokens to types, which is simply the average frequency with which each type is used
> collocations
> concordances

### Chapter 2 math 
* 資訊理論相關簡介:
> 編碼理論ch1-3 http://aries.dyu.edu.tw/~thhu/
> 如何找出劣幣?—簡介訊息與熵的概念 https://web.math.sinica.edu.tw/math_media/d223/22303.pdf
> 聊不來，聊的來?-初探資訊理論 http://kuanchen-blog.logdown.com/posts/333763
>  https://charlesliuyx.github.io/2017/09/11/%E4%BB%80%E4%B9%88%E6%98%AF%E4%BF%A1%E6%81%AF%E7%86%B5%E3%80%81%E4%BA%A4%E5%8F%89%E7%86%B5%E5%92%8C%E7%9B%B8%E5%AF%B9%E7%86%B5/
>  http://interactivepython.org/runestone/static/pythonds/Introduction/toctree.html
>  https://zhuanlan.zhihu.com/p/26486223
> 求通俗解释NLP里的perplexity是什么？ https://www.zhihu.com/question/58482430
上面這連結cover書裡面資訊理論的段落了

### Chapter 3 linguistics 
* 資訊理論相關簡介:
> syntactic: 語法的; semantic: 語義的; phonological: 音位的
> morphology: 詞法/構詞學
  包含變形inflection(e.g.單數變成複數、陰性化陽性化、詞語的"格"), 衍生derivation(e.g.形容詞加上ly變副詞、形容加上en變動詞，改變型態), 複合compounding(結合字，e.g.tea kettle)
* 標記:
> 名詞: NN singular nouns 單數名詞, NNP proper nouns 專有名詞, NR adverbial nouns 副詞性名詞(e.g.home, tomorrow), 
  複數的話在結尾加上S, 所有格的話在結尾加上$
> 限定詞: Determiners 限定詞, 其中的子類別包含 AT articles 冠詞 以及 DT demonstrutives 指示代詞(this, that), DTX double conjunction 聯合限定詞
> 數量詞: Quunlifiers, 表達類似於all, some, many的意思, ABN pre-quantifier 前置性數量詞(all, many) and PN nominal pronoun 名詞性代詞(one, something, anything, somebody).
  The tag for there when used to express existence at the beginning of a sentence is EX
> 形容詞: JJ Adjectives 形容詞, JJR comparatives 比較級, JJT superlatives 最高級, JJS 語義上的最高級(chief, main, top)
  CD cardinals 基數, OD ordinals 序數, 修飾名詞用法被稱作attributive屬性 or adnominal修飾詞,

### Chapter 4 Corpus-Based Work
講了資料來源、程式語言選擇、正規表達式、未/已標註資料、tokenization(判定什麼是詞)的諸多問題e.g.句號不只是句號/怎麼對待縮寫符號/hiphen/同時處理多國語言習慣不同等
Morphology(詞法/構詞問題, 包含stemming)、判定什麼是句子(e.g.現行方法如偵測句子的邊界)、markup(標註文本型態格式、實際標註標籤)

### Chapter 5 Collocations 搭配字詞
##concept
> collocations are characterized by limited compositionality複合構詞法:
  有些collocations可以直接用組成的字義推敲，有些可以得到部分含義，有些則不行
  collocations和以下三個詞的有些概念重疊: term, technical term, terminological phrase
> 應用的地方包含產生自然語言(以免太像機器生成)、計算辭典編撰(找出重要的collocations)、parsing句法分析(才能解析出自然的collocations)、詞庫語言學研究、社會現象研究(e.g.歧視性用語)等
> 傳統結構語言學家不太care這個主題，因為他們想做的事情是歸納句子找出背後的法則，而英國的語言學家就很care這個，他們重視社會與文化的context
  作者舉strong tea當例子, 他們不會用powerful tea雖然意思上相同
> 本章會討論選擇collocations 的幾種方式: 
  selection of collocations based on frequency, mean and variance of the distince between focal word and collocating word, 
  hypothesis testing, mutual information

##frequecy
> 純粹算兩個字(bigram)是最快的方法，但會產生一些爛結果e.g. of the, to be
  修正的方法是filter the patterns of words which are likely to be "phrases"(用引號是因為作者後面要討論phrase ?= collocations)
  譬如說用part-of-speech filter，就是看組成詞性 e.g. AN, NN, AAN, NPN等等, 
  這同時結合quantatative technique(頻率+過濾) and linguistics knowledge(詞性)
  相對應例子是linear regression, regression coefficiennts, Gaussian random variable, degress of freedom
  還有其他應用是找滿足上述詞性規則的最長序列(longest sequence fits the patterns), 
  譬如上面的方法會找到new york, york city, 找最長序列就會找到new york city
> 這邊都是討論fixed phrases, 但上面沒cover到有距離的fixed phrases, 針對這類型的collocations, 
  我們會把上面計算bigrams的做法略作延伸, 定義所謂collocational window(通常是字前後三到四個字為一個window), 
  在window之內計算每個bigram的頻率, 然後就可以用一樣的方法找高頻collocations
  
##concept
> frequency可以解決fixed phrases, 但有些collocations的關係更為彈性, 譬如說knock on/at ... a door, door前面可能會有所有格或是形容詞
  不只是結構不固定(沒有連在一起), 距離也不固定(不只一個詞), 但是模式又夠明顯讓我們知道說knock是正確的動詞而非使用hit
  mean and variance based methods要處理的是可變距離(varying distance)的collocations
> 例子:
  a. she knocked on his door
  b. they knocked at the door
  c. 100 women knocked on Donaldson’s door
  d. a man knocked on the metal front door
  舉例而言, knocked 與 door的距離等同於計算詞庫中兩個詞的偏移量(offset, 代表有符號的距離), 
  mean offset = (3+3+5+5)/4 = 4.0 (計算時將Donaldson’s拆成 D, ', s 三個字),
  若有順序相反的情況(door 出現在 knocked 前面), 距離會被記為負值, 我們將分析限制在中心詞(focal word)前後九個詞的window中
  variance 就是衡量個別的偏移量offsets 與平均的距離, 標準差 = (((3-4)^2 +(3-4)^2 +(5-4)^2 +(5-4)^2)/3)^1/2 ~= 1.15
  我們找低標準差的詞對, 代表兩個詞通常已大致相同的距離出現, 若偏移量隨機分佈, 代表兩個詞偶然一起出現, 不存在特定關係, 則標準差值會較大
  另外需注意有時候會發現找到平均距離為0的高標準差詞對, 這很正常, 代表他們的距離是均勻分布的(+-都有所以平均接近0)
> 此法感謝學者Smajda, 他的做法還會再過濾掉周遭位置上肥胖的峰值, 也就是周圍位置上沒有較低峰值的柱狀圖, 
  目的是抽取重要術語(在此目的下knocked door就不重要, 但在生成自然語言時就重要)
  
##concept
> frequency可以解決fixed phrases, 但有些collocations的關係更為彈性, 譬如說knock on/at ... a door, door前面可能會有所有格或是形容詞
  不只是結構不固定(沒有連在一起), 距離也不固定(不只一個詞), 但是模式又夠明顯讓我們知道說knock是正確的動詞而非使用hit
  mean and variance based methods要處理的是可變距離(varying distance)的collocations  
  
