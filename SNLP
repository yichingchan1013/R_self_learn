## Statistical Natural Language Processing
p
https://dictionary.cambridge.org/zht/%E8%A9%9E%E5%85%B8/%E8%8B%B1%E8%AA%9E/
Chinese: file:///Users/dtseng02/Documents/Dennis/R%E8%AE%80%E6%9C%AC/snlp.pdf
English: file:///Users/dtseng02/Documents/Dennis/R%E8%AE%80%E6%9C%AC/%E7%B5%B1%E8%A8%88%E8%87%AA%E7%84%B6%E8%AA%9E%E8%A8%80%E8%99%95%E7%90%86%E5%9F%BA%E7%A4%8E%E4%B8%AD%E6%96%87%E7%89%88.pdf

### Resource
[foundations of statistical natural language processing]()   

### Chapter 1 Introduction     
原本的語言學會去尋找語言的規則譬如文法，再來會試圖分辨什麼是好的vs.不好的，但嘗試做這件事情面臨到的問題是語言要跟上人類的需求，
所以常常會有文法上的變化，所以前述的規則就很難一體適用，而這本書的做法不僅僅做分拆，更想做的事情其實是找pattern
（跟前面的做法比起來，前者比較像是嘗試找出規範性的規則，但遇到問題了，後者則是比較敘述性，直接看結果），
所以就利用數學算術以及統計跟機率來計算語言的元素

rationalist: 給機器一定的知識以及推理系統就可以像人類一樣
empiricist: 跟r差在程度差距，也假定人有一些基本的運作系統，對怎樣開展推理或運作有特別的偏好，但是相對前者比較抽象/高階，r會直接假設人腦預設了一套語言規則系統，但e只認為人可能會有發現關聯、辨認模式、歸納法等，透過input可能是教學可能是環境耳濡目染，讓人可以學會語言

就NLP而言，r比較偏向給定一些規則接著讓機器運作，e比較偏向給一些大方向/函數，細部的操作/參數則由統計或機器學習決定

E很強調以corpus為背景學習text的意涵/用途（因為沒辦法從真實世界直接學習，所以會看特定文本底下的text）
I-language: language module of the human mind
E-language: data such as texts (the E-language)

r-linguistic competence 語言能力: reflects the knowledge of language structure that is assumed to be in the mind of a native speaker; one can
isolate linguistic competence and describe it in isolation
e-linguistic performance 語言表現: in the world, which is affected by all sorts of things such as memory limitations and distracting
noises in the environment; reject this notion and want to describe actual use of language.

上面大概是講說r認為有所謂的語言能力，人內建能力可以理解語言架構，e則關注實際語言被實際使用的狀況
最後小結論，r就是設定規則（先驗的）看文句，e則是用統計（後驗的）看文句

兩個語言學家應該問的問題
-What kinds of things do people say? 
-What do these things say/ask/request about the world?
看文本內的pattern其實就可以讓我們理解文句的句法架構，所以NLP就在處理這塊，也就回答了第一個問題
又跳回去講R會怎麼回答第一個問題，R會用文法來講，值得注意的是，grammality純粹用來說這句話是不是well-formed，但判斷標準卻不是以人們會不會這樣說來決定，也就是說利用某些規則去評價，但這些規則未必貼合現實（譬如說非母語使用者會講一些文法對的話，但講出來很怪）也就是說，有些句子文法對有些句子文法錯，so what，這個（文法對錯）的資訊到底告訴了我們什麼？（既然不是用現實情況來評價的話）

#語言的non-categorical(categorical: 明確的) 現象
正因為語言變遷普遍來說是隨時間慢慢演化的，因此若要觀察這樣的趨勢不是直接說喔喔這個詞是什麼屬性有什麼用途，較好的作法應該是觀察使用頻率以及和其他詞彙的關係，也就是說應該是用統計方法而非明確的觀察判斷，更激進一點說：把Language and cognition以 probabilistic phenomena看待
批評者說，統計不能告訴我們一段話的意義是什麼，但是作者回應，統計會問要怎麼定義一段話的意義？從統計NLP的角度來看，意義會在於distribution of contexts over which words and utterances are used.也呼應前述的"the meaning of a word is defined by the circumstances of its use"

a practical NLP system must be good at making disambiguation decisions of word sense, word category, syntactic structure, and
semantic scope. But the goal of maximizing coverage while minimizing resultant ambiguity is fundamentally inconsistent with  symbolic NLP
systems, where extending the coverage of the grammar to obscure constructions simply increases the number of undesired parses for common sentences and vice versa.
A Statistical NLP approach seeks to solve these problems by automatically learning lexical and structural preferences from corpora. Rather than parsing solely using syntactic categories, such as part of speech labels, we recognize that there is a lot of information in the relationships between words, that is, which words tend to group with each other.
In particular, the use of statistical models offers a good solution to the ambiguity problem: statistical models are robust,
generalize well, and behave gracefully in the presence of errors and new data.

第一個問題等同問
Rather than starting off by dividing sentences into grammatical and ungrammatical ones, we instead ask, 
“What are the common patterns that occur in language use?” The major tool which we use to identify these patterns is counting things, 
otherwise known as statistics, and so the scientific foundation of the book is found in probability theory.

* 其他語言學家會感興趣的問題   
> How many words are there in the text?
1. How many word tokens there are: 
2. How many different words (how many word types): 
tokens, individual occurrences of something
types, the different things present
the ratio of tokens to types, which is simply the average frequency with which each type is used
> collocations
> concordances

### Chapter 2 math 
* 資訊理論相關簡介:
> 編碼理論ch1-3 http://aries.dyu.edu.tw/~thhu/
> 如何找出劣幣?—簡介訊息與熵的概念 https://web.math.sinica.edu.tw/math_media/d223/22303.pdf
> 聊不來，聊的來?-初探資訊理論 http://kuanchen-blog.logdown.com/posts/333763
>  https://charlesliuyx.github.io/2017/09/11/%E4%BB%80%E4%B9%88%E6%98%AF%E4%BF%A1%E6%81%AF%E7%86%B5%E3%80%81%E4%BA%A4%E5%8F%89%E7%86%B5%E5%92%8C%E7%9B%B8%E5%AF%B9%E7%86%B5/
>  http://interactivepython.org/runestone/static/pythonds/Introduction/toctree.html
>  https://zhuanlan.zhihu.com/p/26486223
> 求通俗解释NLP里的perplexity是什么？ https://www.zhihu.com/question/58482430
上面這連結cover書裡面資訊理論的段落了

### Chapter 3 linguistics 
* 資訊理論相關簡介:
> syntactic: 語法的; semantic: 語義的; phonological: 音位的
> morphology: 詞法/構詞學
  包含變形inflection(e.g.單數變成複數、陰性化陽性化、詞語的"格"), 衍生derivation(e.g.形容詞加上ly變副詞、形容加上en變動詞，改變型態), 複合compounding(結合字，e.g.tea kettle)
* 標記:
> 名詞: NN singular nouns 單數名詞, NNP proper nouns 專有名詞, NR adverbial nouns 副詞性名詞(e.g.home, tomorrow), 
  複數的話在結尾加上S, 所有格的話在結尾加上$
> 限定詞: Determiners 限定詞, 其中的子類別包含 AT articles 冠詞 以及 DT demonstrutives 指示代詞(this, that), DTX double conjunction 聯合限定詞
> 數量詞: Quunlifiers, 表達類似於all, some, many的意思, ABN pre-quantifier 前置性數量詞(all, many) and PN nominal pronoun 名詞性代詞(one, something, anything, somebody).
  The tag for there when used to express existence at the beginning of a sentence is EX
> 形容詞: JJ Adjectives 形容詞, JJR comparatives 比較級, JJT superlatives 最高級, JJS 語義上的最高級(chief, main, top)
  CD cardinals 基數, OD ordinals 序數, 修飾名詞用法被稱作attributive屬性 or adnominal修飾詞,

### Chapter 4 Corpus-Based Work
講了資料來源、程式語言選擇、正規表達式、未/已標註資料、tokenization(判定什麼是詞)的諸多問題e.g.句號不只是句號/怎麼對待縮寫符號/hiphen/同時處理多國語言習慣不同等
Morphology(詞法/構詞問題, 包含stemming)、判定什麼是句子(e.g.現行方法如偵測句子的邊界)、markup(標註文本型態格式、實際標註標籤)

### Chapter 4 Corpus-Based Work
> 

### week8_01/20
> * Learned [**tidystringdist**](https://colinfay.me/tidystringdist/reference/tidy_stringdist.html) packge
> * Digged deeper into [**purrr**](https://colinfay.me/tidystringdist/reference/tidy_stringdist.html) packge, recommended [**使用purrr package學functional programming的觀念**](https://weitinglin.com/tag/purrr/), a series of articles teaching purrr
> * prepared for Dr.Hsieh's Public Opinion Analysis class' research proposal, focusing on cyber/water army detection, read 3 research papers related to the topic
> * finished 50 pages of [*foundations of statistical natural language processing*](http://www.cookbook-r.com/Graphs/)
> * studied half of **SQL Fundamentals for Business Intelligence** on O'reilly Safiri, installed PSQL und used it for practice
> * Read at least 5 articles on [R-bloggers](https://www.cjr.org/index.php) daily
> **Reading**: 不當行為, 活著告訴你

### reflection
> * behavior-based, content-based, network-based approaches   
> * social networks with real identities vs. BBS with anonymous identities
> * paid users in news website/e-commerce review page/micrp-blog platforms/Bulletin Board System/social networks
> * similarities between intenet water army and review spammers, like purpose, behavior patterns, organization, etc.
> * 衝熱度不等於帶風向，想想網軍的發文incentive，中國水軍可能是以帖子計價，台灣網軍可能是以帖子以及風向計價，ID切換類似順序
> **Reading**: 不當行為, 活著告訴你
